{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_Affine/Softmax_계층구현_최적화.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOHJKlVZN0wMHQcaZEXZ73p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["MNIST Dataset load -> hyper parameter setting -> list(grads) 요소 초기화 ->   \n","**forward**(loss 함수가 predict 함수 호출 -> 각 layer마다 forward 함수 수행) -> **backward**(gradient함수로 편미분 완료되면 parameter update)"],"metadata":{"id":"uBuiKnC_u-G9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSnsOC4StjgT"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict"]},{"cell_type":"code","source":["# MNIST Dataset Load\n","try:\n","    import urllib.request\n","except ImportError:\n","    raise ImportError('You should use Python 3.x')\n","import os.path\n","import gzip\n","import pickle\n","import os\n","import numpy as np\n","\n","url_base = 'http://yann.lecun.com/exdb/mnist/'\n","key_file = {\n","    'train_img':'train-images-idx3-ubyte.gz',  # train용 image와 label 각각 60000개\n","    'train_label':'train-labels-idx1-ubyte.gz',\n","    'test_img':'t10k-images-idx3-ubyte.gz',    # test용 image와 label 각각 10000장\n","    'test_label':'t10k-labels-idx1-ubyte.gz'\n","}\n","\n","dataset_dir = os.path.dirname(os.path.abspath(''))\n","save_file = dataset_dir + \"/mnist.pkl\"\n","\n","train_num = 60000\n","test_num = 10000\n","img_dim = (1, 28, 28)\n","img_size = 784\n","\n","\n","def _download(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","    \n","    if os.path.exists(file_path):\n","        return\n","\n","    print(\"Downloading \" + file_name + \" ... \")\n","    urllib.request.urlretrieve(url_base + file_name, file_path)\n","    print(\"Done\")\n","   \n","def download_mnist():\n","    for v in key_file.values():\n","       _download(v)\n","        \n","def _load_label(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","    \n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n","    print(\"Done\")\n","    \n","    return labels\n","\n","def _load_img(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","    \n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n","    with gzip.open(file_path, 'rb') as f:\n","            data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    data = data.reshape(-1, img_size)\n","    print(\"Done\")    \n","    return data\n","    \n","def _convert_numpy():\n","    dataset = {}\n","    dataset['train_img'] =  _load_img(key_file['train_img'])\n","    dataset['train_label'] = _load_label(key_file['train_label'])    \n","    dataset['test_img'] = _load_img(key_file['test_img'])\n","    dataset['test_label'] = _load_label(key_file['test_label'])    \n","    return dataset\n","\n","def init_mnist():\n","    download_mnist()\n","    dataset = _convert_numpy()\n","    print(\"Creating pickle file ...\")\n","    with open(save_file, 'wb') as f:  # pickle 파일 가져와 쓰듯, 학습한 데이터를 weight file로 저장해 재사용할 수 있다\n","        pickle.dump(dataset, f, -1)\n","    print(\"Done!\")\n","\n","def _change_one_hot_label(X):\n","    T = np.zeros((X.size, 10))\n","    for idx, row in enumerate(T):\n","        row[X[idx]] = 1        \n","    return T    \n","\n","def load_mnist(normalize=True, flatten=True, one_hot_label=False): # False? load_mnist에 원핫인코딩된 상태로 오기 때문(line 63)"],"metadata":{"id":"pJRFoOd_t4wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyper Parameter Setting\n","    \"\"\"MNIST 데이터셋 읽기\n","    \n","    Parameters\n","    ----------\n","    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n","    one_hot_label : \n","        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n","        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n","    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n","    \n","    Returns\n","    -------\n","    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n","    \"\"\"\n","    if not os.path.exists(save_file):\n","        init_mnist()\n","        \n","    with open(save_file, 'rb') as f:\n","        dataset = pickle.load(f)\n","    \n","    if normalize:\n","        for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].astype(np.float32)\n","            dataset[key] /= 255.0  # 흑백 이미지는 보통 8bit \n","            \n","    if one_hot_label:\n","        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n","        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n","    \n","    if not flatten:\n","         for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n","\n","    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "],"metadata":{"id":"A9sIeQi0t40R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 함수(원소 초기화 -> forward -> backward)\n","# 클래스(활성화 함수는 4장에서 Sigmoid, 5장에서 ReLU 사용)\n","'''\n","신경망 구성을 위한 함수와 클래스 구현.\n","'''\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","        return dx\n","\n","class Affine: # Matrix와 bias 한꺼번에 처리\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        \n","        self.x = None\n","        self.original_x_shape = None\n","        # 가중치와 편향 매개변수의 미분\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # 텐서 대응\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        self.x = x\n","\n","        out = np.dot(self.x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실함수\n","        self.y = None    # softmax의 출력\n","        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n","        \n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n","            dx = (self.y - self.t) / batch_size\n","        else:\n","            dx = self.y.copy()\n","            dx[np.arange(batch_size), self.t] -= 1\n","            dx = dx / batch_size\n","      \n","        return dx"],"metadata":{"id":"BD2O7u4Rt45m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MODEL = NETWORK\n","'''\n","4장에서 구현한 2층 신경망을 오차역전파법을 사용하여 학습을 구현함.\n","'''\n","class TwoLayerNet:\n","\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # 계층 생성 : affine, relu class의 인스턴스를 생성하여 dict에 넣는다 = method나 attribution을 사용하겠다\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","\n","        self.lastLayer = SoftmaxWithLoss()\n","        # Affine1 -> Relu1 -> Affine2 -> Softmax 의 레이어를 갖는 신경망을 구현.\n","        \n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)        \n","        return x\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    # 4장 : GRADIENT 2개(수치미분, Back Propagation) -> 5장 : 수치미분은 속도가 느려, Back Propagation만 사용     \n","    def gradient(self, x, t): # update하기 전까지는 gradient에 모두 묶여 있다.(loss(<-forward) + backward)\n","        # loss는 forward와 관련된 것이 모두 묶여있는 것\n","        self.loss(x, t) \n","\n","        # backward\n","        dout = 1                                # 역전파의 초기 신호를 1으로 설정,\n","        dout = self.lastLayer.backward(dout)    # 역전파를 계산하여 해당 레이어 클래스에 기울기(Gradient)를 저장.\n","        \n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","        return grads\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","#(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\n","#t_train=tf.keras.utils.to_categorical(t_train)\n","#t_test=tf.keras.utils.to_categorical(t_test)\n","#x_train=x_train.reshape(60000,-1)\n","#x_test=x_test.reshape(10000,-1)"],"metadata":{"id":"3HBpX-drt48o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SGD 수행한 후, 편미분값으로 parameter update(갱신)\n","iters_num = 10000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    # 기울기 계산\n","    grad = network.gradient(x_batch, t_batch)\n","    \n","    # 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]    # 학습 후 계산한 기울기(Gradient)로 경사하강법을 사용하여 학습.\n","    \n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","    \n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"epoch :\", int(i / iter_per_epoch))\n","        print(\"train_acc : %.4f\" % (train_acc))\n","        print(\"test_acc : %.4f\" % (test_acc))\n","        print()\n","\n","# draw acc graph.\n","x = np.arange(1, len(train_acc_list) + 1)\n","plt.plot(x, train_acc_list, label='train Acc')\n","plt.plot(x, test_acc_list, label='test Acc', linestyle='--')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.ylim(0, 1.0)\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"OUGa451At4_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"l3JiigOLt5FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TKZbmL1Mt5He"},"execution_count":null,"outputs":[]}]}